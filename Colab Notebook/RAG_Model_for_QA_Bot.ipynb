{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PART - 1\n",
        "\n",
        "# Retrieval-Augmented Generation (RAG) Model for QA Bot\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> Problem Statement: Develop a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA) bot for a business.\n",
        "\n",
        "\n",
        "> Use a vector database like Pinecone DB and a generative model like Cohere API (or any other available alternative). The QA bot should be able to retrieve relevant information from a dataset and generate coherent answers.\n",
        "\n",
        "\n",
        "\n",
        "> Task Requirements:\n",
        "1. Implement a RAG-based model that can handle questions related to a provided document or dataset.\n",
        "2. Use a vector database (such as Pinecone) to store and retrieve document embeddings efficiently.\n",
        "3. Test the model with several queries and show how well it retrieves and generates accurate answers from the document.\n",
        "\n",
        "\n",
        "\n",
        "> Deliverables:\n",
        "1. A Colab notebook demonstrating the entire pipeline, from data loading to\n",
        "question answering.\n",
        "2. Documentation explaining the model architecture, approach to retrieval, and how generative responses are created.\n",
        "3. Provide several example queries and the corresponding outputs"
      ],
      "metadata": {
        "id": "ZzknqlkSGXyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Installing the required libraries\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "72Xytg-LGjIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explaination:\n",
        "This instruction will set up three Python packages: `pinecone-client`, `cohere`, and `transformers`. Here is an analysis of the functions of each library.\n",
        "\n",
        "1. **`pinecone-client`**: a client specifically for pinecone.\n",
        "\n",
        "- **Objective**: This serves as the authorized platform for the Pinecone vector database, an efficiently optimized system for storing, organizing, and searching vectors (utilized for similarity searches). In retrieval-augmented generation (RAG) models, it is often used to efficiently manage large datasets or documents by evaluating vectorized text representations.\n",
        "\n",
        "- **Application Scenario**: The storage of document embeddings for the purpose of conducting similarity searches in order to access pertinent information.\n",
        "\n",
        "2. **`cohere`**:\n",
        "\n",
        "- **Objective**: Cohere provides robust natural language processing (NLP) APIs, such as text generation and embeddings. Installing this package allows you to utilize Cohere's models for creating embeddings for documents and queries, as well as generating text from prompts.\n",
        "\n",
        "- **Application**: Commonly employed in QA systems or chatbots to produce text or embeddings that can be searched in vector databases such as Pinecone.\n",
        "\n",
        "3. **`transformers`**:\n",
        "\n",
        "**Objective**: Hugging Face's library was created to allow simple usage of cutting-edge deep learning models, mainly transformer-based ones like GPT, BERT, etc. It is utilized for a range of NLP activities like text categorization, language conversion, summarization, and additional tasks.\n",
        "\n",
        "- **Application Scenario**: Incorporating transformer-based models into your workflow, like creating embeddings or providing direct responses in a QA system.\n",
        "\n",
        "These libraries work together to assist in creating a sophisticated pipeline for tasks such as developing a Retrieval-Augmented Generation (RAG) model. Pinecone deals with the fetching (utilizing vector similarity), Cohere assists in text embeddings and generation, and transformers offer extra NLP capabilities."
      ],
      "metadata": {
        "id": "d5EPubBqIFIN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXKFLt-hBIae",
        "outputId": "4fe48117-3cf7-4317-83d2-104a0358e46d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting cohere\n",
            "  Downloading cohere-5.9.2-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.8.30)\n",
            "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Collecting boto3<2.0.0,>=1.34.0 (from cohere)\n",
            "  Downloading boto3-1.35.19-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx>=0.21.2 (from cohere)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.9.1)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.23.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.19.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20240914-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Collecting botocore<1.36.0,>=1.35.19 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading botocore-1.35.19-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx>=0.21.2->cohere)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.21.2->cohere)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.19->boto3<2.0.0,>=1.34.0->cohere) (2.8.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.19->boto3<2.0.0,>=1.34.0->cohere) (1.16.0)\n",
            "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.9.2-py3-none-any.whl (222 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.4/222.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading boto3-1.35.19-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading types_requests-2.32.0.20240914-py3-none-any.whl (15 kB)\n",
            "Downloading botocore-1.35.19-py3-none-any.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: types-requests, pinecone-plugin-interface, parameterized, jmespath, httpx-sse, h11, fastavro, pinecone-plugin-inference, httpcore, botocore, s3transfer, pinecone-client, httpx, boto3, cohere\n",
            "Successfully installed boto3-1.35.19 botocore-1.35.19 cohere-5.9.2 fastavro-1.9.7 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 httpx-sse-0.4.0 jmespath-1.0.1 parameterized-0.9.0 pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7 s3transfer-0.10.2 types-requests-2.32.0.20240914\n"
          ]
        }
      ],
      "source": [
        "!pip install pinecone-client cohere transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing the **Pinecone client** with the command `!pip install pinecone` enables efficient storage and querying of vectors (numerical representations of data). Pinecone is frequently utilized for similarity searches, particularly in areas such as information retrieval, machine learning, and QA systems. Comparing vectors aids in quickly fetching pertinent information, commonly paired with models such as BERT or GPT for complex NLP assignments."
      ],
      "metadata": {
        "id": "KIViFiZHJCkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edu33wrxBpr6",
        "outputId": "82b6b661-f653-474d-bd7a-34754c2b28d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pinecone\n",
            "  Downloading pinecone-5.1.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2024.8.30)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from pinecone) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.0.7)\n",
            "Downloading pinecone-5.1.0-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.5/245.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pinecone\n",
            "Successfully installed pinecone-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Importing the Pinecone and Cohere\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "54aNhTCgGsep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **`import Pinecone from pinecone`**:\n",
        "\n",
        "- Importing the `Pinecone` class from the `pinecone` library enables interaction with Pinecone’s vector database.\n",
        "\n",
        "2. **`pc = Pinecone(api_key=\"b10a42ec-9e36-4523-a2e1-08ce6de826f6\")`**:\n",
        "\n",
        "- A Pinecone client instance is created by this code snippet with the specified API key (`b10a42ec-9e36-4523-a2e1-08ce6de826f6`). The API key verifies your permission to use the Pinecone service, enabling you to establish and oversee indexes (collections of vectors).\n",
        "\n",
        "3. **`pc.Index(\"index384\")`** is used to set the variable index.\n",
        "\n",
        "- This code accesses or makes a Pinecone index called `\"index384\"`. A vector database is an index where vectors (embeddings) are stored and managed. This index allows you to insert, retrieve, and search for vector embeddings using similarity.\n",
        "\n",
        "Overview:\n",
        "\n",
        "- By utilizing your API key, the code establishes a connection with Pinecone and operates on an index labeled `\"index384\"` for handling and searching vector data for operations such as similarity search."
      ],
      "metadata": {
        "id": "PL6KAZ13G2iU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "pc = Pinecone(api_key=\"b10a42ec-9e36-4523-a2e1-08ce6de826f6\")\n",
        "index = pc.Index(\"index384\")"
      ],
      "metadata": {
        "id": "MrJDZwNUBtOY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the code:\n",
        "\n",
        "1. **`import cohere`**:\n",
        "\n",
        "- This code includes the `cohere` library for utilizing the Cohere API in tasks such as text generation, embeddings, and various NLP operations.\n",
        "\n",
        "2. **`import AutoTokenizer and AutoModel from transformers.`**:\n",
        "\n",
        "- The `transformers` library from Hugging Face is utilized to import the `AutoTokenizer` and `AutoModel` classes for text tokenization and utilizing pre-trained models such as BERT, GPT, etc.\n",
        "\n",
        "3. **`import in torch.`**\n",
        "\n",
        "- PyTorch is brought into use, primarily for manipulating tensors and constructing machine learning models.\n",
        "\n",
        "4. The cohere_api_key is \"LgaWUuuPnuamPELt1VTEqP6WmwEYOjfLKFrsUg6P\".\n",
        "\n",
        "- This line is the definition of the API key required for authenticating requests to Cohere's service. This is the method through which the customer links to the Cohere platform.\n",
        "\n",
        "5. **`co = cohere.Client(using_co_here_api_key)`**:\n",
        "\n",
        "- By using the given API key, you can set up a Cohere client to access services such as generating text and creating embeddings.\n",
        "\n",
        "Synopsis:\n",
        "\n",
        "This code initializes the essential resources for an NLP pipeline by bringing in the `cohere` library for Cohere's API interaction, the `transformers` library for managing pre-trained models, and `torch` for machine learning tasks. The Cohere API key is required to verify the client's identity and access Cohere's services."
      ],
      "metadata": {
        "id": "Eq0Cd6VFHIn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "cohere_api_key = \"LgaWUuuPnuamPELt1VTEqP6WmwEYOjfLKFrsUg6P\"\n",
        "co = cohere.Client(cohere_api_key)"
      ],
      "metadata": {
        "id": "09t087p-BYFI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Loading the Tokenizer and Model : sentence-transformers/all-MiniLM-L6-v2\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Explanation of the Code:\n",
        "\n",
        "1. **`tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')`** is used to initialize a tokenizer from a pre-trained model.\n",
        "\n",
        "- Initializes the tokenizer for the `sentence-transformers/all-MiniLM-L6-v2` model. Tokenizers alter text into a structure that can be understood by the model, such as token IDs.\n",
        "\n",
        "2. **`model`** is created by loading a pretrained model called 'sentence-transformers/all-MiniLM-L6-v2'.\n",
        "\n",
        "- Imports the pre-trained model `all-MiniLM-L6-v2` from Hugging Face's library `sentence-transformers`. This model is employed to create text embeddings.\n",
        "\n",
        "3. **`define embed_text(text):`**:\n",
        "\n",
        "Creates a function called `embed_text` that accepts text as input and gives back its embeddings.\n",
        "\n",
        "4. **`The tokenizer processes the text with padding and truncation and returns a PyTorch tensor.`**:\n",
        "\n",
        "The input text is tokenized to create tensors for PyTorch ('pt'). By setting `padding=True`, all sequences are made the same length, and `truncation=True` shortens overly long sequences.\n",
        "\n",
        "5. **`model(**inputs)`** is used to obtain outputs.\n",
        "\n",
        "Runs the tokenized inputs through the model to obtain the output embeddings.\n",
        "\n",
        "6. **`embeddings = mean of outputs last hidden state along dimension 1`**:\n",
        "\n",
        "- It takes the final hidden state of the model and uses mean pooling on the token axis to generate a singular vector that encapsulates the entire text.\n",
        "\n",
        "7. **`detach()`** method is used to create a new tensor detached from the computational graph.\n",
        "\n",
        "Transforms the embeddings into a NumPy array (disconnected from PyTorch's computational graph) and provides the initial element of the array. This is the representation of the input text.\n",
        "\n",
        "The passage describes how to effectively paraphrase a text in the same language while maintaining the original word count and meaning.\n",
        "\n",
        "The code creates a function that produces text embeddings utilizing the `all-MiniLM-L6-v2` model. The input text is tokenized, fed through the model, and average token embeddings are extracted from it. The outcome is a vector of a specific size that represents the text and can be used for tasks like similarity search or clustering."
      ],
      "metadata": {
        "id": "WtIDSj0fKYQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "def embed_text(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "    outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    return embeddings.detach().numpy()[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxvn3682CQms",
        "outputId": "6f7691da-80ca-42f7-f398-05466adaaea7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Loading the Document and functioning to Embed and Store the Document in Pinecone\n",
        "\n",
        "---\n",
        "\n",
        "### Analysis of the Code:\n",
        "\n",
        "1. **`documents`**:\n",
        "\n",
        "This list includes multiple documents on Artificial Intelligence (AI) and Machine Learning (ML). Every input is a string that portrays a distinct feature of artificial intelligence and machine learning.\n",
        "\n",
        "2. **`Iterating through the documents using indexing i and document variable doc`**:\n",
        "\n",
        "- This iteration goes through every document in the `documents` list. `i` corresponds to the document's index, while `doc` represents the document's text.\n",
        "\n",
        "\n",
        "3. **`vectorize the text document using the embed_text function`**:\n",
        "\n",
        "Uses the `embed_text` function (previously defined) to produce an embedding (a vector representation) for the present document.\n",
        "\n",
        "5. **`index.upsert(vectors=[(str(i), vector)])`** can be used to add or update a vector in the index.\n",
        "\n",
        "- Places (or modifies) the document embedding in the Pinecone index. The `index.upsert` function requires a list of pairs, with each pair consisting of a distinct ID (in this situation, the string form of index `i`) and the associated vector (embedding) for the document.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "The main points are...\n",
        "\n",
        "The code processes a list of documents by creating embeddings for each document with the `embed_text` function, and then saves these embeddings in the Pinecone vector index. Every record in the Pinecone database has a specific ID (its position in the list) that enables quick similarity searches and retrieval using these embeddings."
      ],
      "metadata": {
        "id": "vvr6vkjBLVPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"Artificial Intelligence (AI) and Machine Learning (ML) are rapidly evolving fields that have a significant impact on various industries. This document provides an overview of AI and ML, their applications, and some common challenges associated with these technologies.\",\n",
        "    \"Artificial Intelligence refers to the simulation of human intelligence in machines that are programmed to think and learn. AI systems can perform tasks such as speech recognition, decision-making, and problem-solving.\",\n",
        "    \"Key areas of AI include: Natural Language Processing (NLP): Enables machines to understand and interact with human language.Computer Vision: Allows machines to interpret and process visual information from the world.Robotics: Involves designing and building robots that can perform tasks autonomously.\",\n",
        "    \"Machine Learning is a subset of AI that involves training algorithms to learn from and make predictions or decisions based on data.\",\n",
        "    \"Machine learning models can be categorized into:Supervised Learning: Models are trained on labeled data to predict outcomes.Unsupervised Learning: Models identify patterns and relationships in unlabeled data.Reinforcement Learning: Models learn to make decisions through trial and error to maximize rewards.\",\n",
        "    \"AI and ML are used in various applications, including recommendation systems (e.g., Netflix), virtual assistants (e.g., Siri), autonomous vehicles, and medical diagnosis.\",\n",
        "    \"Common challenges of AI and ML include data privacy concerns, algorithmic bias, and the need for large datasets to train accurate models.\",\n",
        "    \"Businesses can leverage AI and ML to automate processes, gain insights from data, improve customer experiences, and drive innovation.\"\n",
        "]\n",
        "\n",
        "# Embed and store documents in Pinecone\n",
        "for i, doc in enumerate(documents):\n",
        "    vector = embed_text(doc)\n",
        "    index.upsert(vectors=[(str(i), vector)])\n"
      ],
      "metadata": {
        "id": "Xa95iuLRCTiH"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Retrieve Document Section\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### Analysis of the Code:\n",
        "\n",
        "1. **`def get_documents(query, top_k=3):`**:\n",
        "\n",
        "Creates a function called `retrieve_documents` which accepts a `query` string and an optional parameter `top_k` (default is 3) to indicate the number of top matches to fetch.\n",
        "\n",
        "2. Obtain embedding of the query text by using the function `embed_text(query)`.\n",
        "\n",
        "3. Creates an embedding for the query by utilizing the specified `embed_text` function that was defined earlier.\n",
        "\n",
        "- Ensure that the length of query_embedding is equal to 384, otherwise raise an error message indicating the incorrect dimensions.\n",
        "\n",
        "- Guarantees the query embedding produced has the accurate dimension (384). If the size is not right, an error will be triggered. (Note: The size of this dimension needs to align with the dimensions of the embeddings in Pinecone; adjustments may be necessary depending on the actual embedding size.)\n",
        "\n",
        "4. Convert `query_embedding` to a list.\n",
        "\n",
        "- Transforms the query embedding from a NumPy array into a list structure. Pinecone queries typically need the vector to be in list format.\n",
        "\n",
        "5. **`results`** represents the output of running the **`index.query`** function with parameters **`query_embedding`** and **`top_k`**.\n",
        "\n",
        "- Utilizes the query embedding to search the Pinecone index and collect the top `top_k` most similar vectors (documents). This conducts a search for documents that are most similar to the query.\n",
        "\n",
        "6. Get the relevant documents by selecting them based on the IDs provided in the matches within the results.\n",
        "\n",
        "- Matching vectors and their corresponding IDs in `results['matches']` are compared to the original `documents` list to extract relevant documents, with the original document being retrieved based on the ID using `int(match['id'])`.\n",
        "\n",
        "7. **`give back relevant_docs`**:\n",
        "\n",
        "- Provides a selection of the most pertinent files according to the search query.\n",
        "\n",
        "Summary:\n",
        "\n",
        "Please rewrite the text you would like paraphrased.\n",
        "\n",
        "The function `retrieve_documents` accepts a query, transforms it into an embedding, and then looks for the most similar documents in the Pinecone index. It fetches and gives back the top `top_k` documents that have the highest similarity to the query."
      ],
      "metadata": {
        "id": "Ca0AUBrgObe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_documents(query, top_k=3):\n",
        "    query_embedding = embed_text(query)\n",
        "\n",
        "    assert len(query_embedding) == 384, f\"Query embedding has incorrect dimensions: {len(query_embedding)}\"\n",
        "\n",
        "    query_embedding = query_embedding.tolist()\n",
        "\n",
        "    results = index.query(vector=query_embedding, top_k=top_k)\n",
        "\n",
        "    relevant_docs = [documents[int(match['id'])] for match in results['matches']]\n",
        "    return relevant_docs"
      ],
      "metadata": {
        "id": "c4EeJS1EDBmG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Generate Answer Section\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Explanation of the code:\n",
        "\n",
        "1. **`def create_response(relevant_documents, search_query):`**:\n",
        "\n",
        "Creates a function called `generate_answer` that accepts a list of `relevant_docs` (the documents obtained when querying) along with the query string.\n",
        "\n",
        "2. **`context`** combines the relevant documents into a single string.\n",
        "\n",
        "- Joins the array of pertinent records to create a unified string named `context`. This context will be utilized to offer pertinent background details in order to formulate a response.\n",
        "\n",
        "3. **`response = co.create()`**.\n",
        "\n",
        "- Invokes the `generate` function from the Cohere API to create a response. Below is an outline of the parameters that were utilized:\n",
        "\n",
        "- **`model='command-xlarge-nightly'`**: Indicates the specific model to be utilized for generating text. `command-xlarge-nightly` is a model developed by Cohere for creating text in response to prompts.\n",
        "\n",
        "- **`prompt=f\"Combine the provided context with the question to form your answer: {context}\\n\\nQuestion: {query}\"`**: Combines the context and the question to create the prompt for answering. This gives the model the essential data to produce a pertinent response.\n",
        "\n",
        "- **`max_tokens=100`**: Restricts the output to a maximum of 100 tokens (words or partial words).\n",
        "\n",
        "- **`temperature=0.7`**: Regulates how inventive the answer can be. A balance between randomness and coherence is achieved with a value of 0.7.\n",
        "\n",
        "4. **`return the text of the first generation in the response`**:\n",
        "\n",
        "- It retrieves and gives back the content of the initial produced reply from the Cohere API. Accessing the generated text from the response is done through `response.generations[0].text`.\n",
        "\n",
        "### Recap:\n",
        "\n",
        "- The `generate_answer` function merges relevant documents and a query into a single context string, then utilizes Cohere’s API to produce an answer from this context. The generated text is provided as the response to the query."
      ],
      "metadata": {
        "id": "fjsp-28jPT4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(relevant_docs, query):\n",
        "    # Concatenate the retrieved docs to form context\n",
        "    context = \" \".join(relevant_docs)\n",
        "\n",
        "    # Cohere's Generate API\n",
        "    response = co.generate(\n",
        "        model='command-xlarge-nightly',\n",
        "        prompt=f\"Answer the question based on the context: {context}\\n\\nQuestion: {query}\",\n",
        "        max_tokens=100,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return response.generations[0].text\n"
      ],
      "metadata": {
        "id": "afLyhiV3Csny"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Result Generation\n",
        "\n",
        "---\n",
        "\n",
        "### Analysis of Code:\n",
        "\n",
        "query = \"Can you explain what Machine Learning is?\"\n",
        "\n",
        "- Describes a user's inquiry regarding Machine Learning.\n",
        "\n",
        "2. **`retrieved_documents = get_documents(query)`**:\n",
        "\n",
        "- Utilizes the `retrieve_documents` function with the user query to fetch the most pertinent documents from Pinecone according to the query embedding.\n",
        "\n",
        "3. **Call the function \"generate_answer\" with the parameters \"retrieved_docs\" and \"query\" and assign the result to the variable \"answer\"**.\n",
        "\n",
        "- Uses Cohere's API to generate an answer by calling the `generate_answer` function with the fetched documents and the initial query.\n",
        "\n",
        "4. **`print(\"Search query:\", query)`**:\n",
        "\n",
        "Displays the user's original query.\n",
        "\n",
        "5. **`display the retrieved documents:`**\n",
        "\n",
        "Produces the papers obtained from Pinecone that correspond to the search.\n",
        "\n",
        "6. **`showing the generated answer with the print function: print(\"Generated Answer:\", answer)`**:\n",
        "\n",
        "Produces the response from Cohere's API using the gathered documents and the query.\n",
        "\n",
        "### Brief Summary:\n",
        "\n",
        "This code runs a pipeline for searching and generating responses. A user query is used to find relevant documents from Pinecone, generate an answer with Cohere, and display the query, documents, and answer. This is beneficial for developing a QA system that generates answers from a collection of documents according to user inquiries."
      ],
      "metadata": {
        "id": "S4OvoTlEP8Vn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Question** : What is Machine Learning?\n",
        "\n",
        "**Answer Generated**: Machine Learning (ML) is a branch of Artificial Intelligence (AI) that focuses on developing algorithms and models which enable computers to learn and make predictions or decisions from data. In supervised learning, ML models are trained using labeled data, where the desired output is provided along with the input data. These models learn to map inputs to outputs and can then make predictions on new, unseen data. Unsupervised learning, on the other hand, involves training models with unlabeled data, allowing them to discover underlying patterns"
      ],
      "metadata": {
        "id": "driF1aC4QDPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User query\n",
        "query = \"What is Machine Learning?\"\n",
        "retrieved_docs = retrieve_documents(query)\n",
        "answer = generate_answer(retrieved_docs, query)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"Retrieved Docs:\", retrieved_docs)\n",
        "print(\"Generated Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVRw7WyeDGMg",
        "outputId": "483bc593-808b-4f58-c090-3daa3c800ddd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is Machine Learning?\n",
            "Retrieved Docs: ['Machine Learning is a subset of AI that involves training algorithms to learn from and make predictions or decisions based on data.', 'Machine learning models can be categorized into:Supervised Learning: Models are trained on labeled data to predict outcomes.Unsupervised Learning: Models identify patterns and relationships in unlabeled data.Reinforcement Learning: Models learn to make decisions through trial and error to maximize rewards.', 'Artificial Intelligence (AI) and Machine Learning (ML) are rapidly evolving fields that have a significant impact on various industries. This document provides an overview of AI and ML, their applications, and some common challenges associated with these technologies.']\n",
            "Generated Answer: Machine Learning (ML) is a branch of Artificial Intelligence (AI) that focuses on developing algorithms and models which enable computers to learn and make predictions or decisions from data. In supervised learning, ML models are trained using labeled data, where the desired output is provided along with the input data. These models learn to map inputs to outputs and can then make predictions on new, unseen data. Unsupervised learning, on the other hand, involves training models with unlabeled data, allowing them to discover underlying patterns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Question** : What is Artificial Intelligence?\n",
        "\n",
        "**Answer Generated** : Artificial Intelligence (AI) is a field of computer science that focuses on creating intelligent systems and machines that can perform tasks and solve problems that typically require human intelligence. It involves developing algorithms and models that enable machines to perceive, reason, learn, and make decisions. AI aims to replicate and enhance human cognitive abilities in machines, enabling them to understand and interpret complex data, recognize patterns, and perform tasks with a high degree of accuracy and efficiency."
      ],
      "metadata": {
        "id": "hvC2UcBnQZJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User query\n",
        "query = \"What is Artificial Intelligence?\"\n",
        "retrieved_docs = retrieve_documents(query)\n",
        "answer = generate_answer(retrieved_docs, query)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"Retrieved Docs:\", retrieved_docs)\n",
        "print(\"Generated Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GRa7_zgFG_6",
        "outputId": "609867c3-babf-419e-b167-3f3fc7fea6a1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is Artificial Intelligence?\n",
            "Retrieved Docs: ['Artificial Intelligence refers to the simulation of human intelligence in machines that are programmed to think and learn. AI systems can perform tasks such as speech recognition, decision-making, and problem-solving.', 'Machine Learning is a subset of AI that involves training algorithms to learn from and make predictions or decisions based on data.', 'Key areas of AI include: Natural Language Processing (NLP): Enables machines to understand and interact with human language.Computer Vision: Allows machines to interpret and process visual information from the world.Robotics: Involves designing and building robots that can perform tasks autonomously.']\n",
            "Generated Answer: Artificial Intelligence (AI) is a field of computer science that focuses on creating intelligent systems and machines that can perform tasks and solve problems that typically require human intelligence. It involves developing algorithms and models that enable machines to perceive, reason, learn, and make decisions. AI aims to replicate and enhance human cognitive abilities in machines, enabling them to understand and interpret complex data, recognize patterns, and perform tasks with a high degree of accuracy and efficiency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Question** : What are key areas of AI?\n",
        "\n",
        "**Answer Generated** : The answer is Key areas of AI include Natural Language Processing (NLP), Computer Vision, and Robotics."
      ],
      "metadata": {
        "id": "77bhO-QDRgiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User query\n",
        "query = \"What are key areas of AI?\"\n",
        "retrieved_docs = retrieve_documents(query)\n",
        "answer = generate_answer(retrieved_docs, query)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"Retrieved Docs:\", retrieved_docs)\n",
        "print(\"Generated Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEOqOhgoFMBo",
        "outputId": "7d7198c2-444a-496c-ac2a-a929a8652783"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What are key areas of AI?\n",
            "Retrieved Docs: ['Key areas of AI include: Natural Language Processing (NLP): Enables machines to understand and interact with human language.Computer Vision: Allows machines to interpret and process visual information from the world.Robotics: Involves designing and building robots that can perform tasks autonomously.', 'AI and ML are used in various applications, including recommendation systems (e.g., Netflix), virtual assistants (e.g., Siri), autonomous vehicles, and medical diagnosis.', 'Artificial Intelligence refers to the simulation of human intelligence in machines that are programmed to think and learn. AI systems can perform tasks such as speech recognition, decision-making, and problem-solving.']\n",
            "Generated Answer: The answer is Key areas of AI include Natural Language Processing (NLP), Computer Vision, and Robotics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Question** : What are the catagories of Machine Learning Models?\n",
        "\n",
        "**Answer Generated** : The answer is Machine Learning models can be categorized into Supervised Learning, Unsupervised Learning, and Reinforcement Learning."
      ],
      "metadata": {
        "id": "NEdkOIuSRsdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User query\n",
        "query = \"What are the catagories of Machine Learning models?\"\n",
        "retrieved_docs = retrieve_documents(query)\n",
        "answer = generate_answer(retrieved_docs, query)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"Retrieved Docs:\", retrieved_docs)\n",
        "print(\"Generated Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr3u5X7QFTyC",
        "outputId": "db720977-2d08-480e-98cf-826c76c4ad5e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What are the catagories of Machine Learning models?\n",
            "Retrieved Docs: ['Machine learning models can be categorized into:Supervised Learning: Models are trained on labeled data to predict outcomes.Unsupervised Learning: Models identify patterns and relationships in unlabeled data.Reinforcement Learning: Models learn to make decisions through trial and error to maximize rewards.', 'Machine Learning is a subset of AI that involves training algorithms to learn from and make predictions or decisions based on data.', 'AI and ML are used in various applications, including recommendation systems (e.g., Netflix), virtual assistants (e.g., Siri), autonomous vehicles, and medical diagnosis.']\n",
            "Generated Answer: The answer is Machine Learning models can be categorized into Supervised Learning, Unsupervised Learning, and Reinforcement Learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Question** : Applications of AI?\n",
        "\n",
        "**Answer Generated** : The answer is Applications of AI include recommendation systems (e.g., Netflix), virtual assistants (e.g., Siri), autonomous vehicles, and medical diagnosis."
      ],
      "metadata": {
        "id": "rVAYHAHVR6KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User query\n",
        "query = \"Applications of AI?\"\n",
        "retrieved_docs = retrieve_documents(query)\n",
        "answer = generate_answer(retrieved_docs, query)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"Retrieved Docs:\", retrieved_docs)\n",
        "print(\"Generated Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-GC1efeFcwQ",
        "outputId": "e9c4aa3a-79e4-4e57-c8d9-b8f6d342622f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Applications of AI?\n",
            "Retrieved Docs: ['Key areas of AI include: Natural Language Processing (NLP): Enables machines to understand and interact with human language.Computer Vision: Allows machines to interpret and process visual information from the world.Robotics: Involves designing and building robots that can perform tasks autonomously.', 'AI and ML are used in various applications, including recommendation systems (e.g., Netflix), virtual assistants (e.g., Siri), autonomous vehicles, and medical diagnosis.', 'Artificial Intelligence refers to the simulation of human intelligence in machines that are programmed to think and learn. AI systems can perform tasks such as speech recognition, decision-making, and problem-solving.']\n",
            "Generated Answer: The answer is Applications of AI include recommendation systems (e.g., Netflix), virtual assistants (e.g., Siri), autonomous vehicles, and medical diagnosis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Question** : Challenges of AI and ML?\n",
        "\n",
        "**Answer Generated** : Challenges of AI and ML include data privacy concerns, algorithmic bias, and the need for large datasets to train accurate models."
      ],
      "metadata": {
        "id": "OCygmBPsR-eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User query\n",
        "query = \"Challenges of AI and ML?\"\n",
        "retrieved_docs = retrieve_documents(query)\n",
        "answer = generate_answer(retrieved_docs, query)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"Retrieved Docs:\", retrieved_docs)\n",
        "print(\"Generated Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOlCo9Q8FkvX",
        "outputId": "1bac4b5c-6f09-4c15-9f6a-644e0644ea40"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Challenges of AI and ML?\n",
            "Retrieved Docs: ['Common challenges of AI and ML include data privacy concerns, algorithmic bias, and the need for large datasets to train accurate models.', 'Businesses can leverage AI and ML to automate processes, gain insights from data, improve customer experiences, and drive innovation.', 'AI and ML are used in various applications, including recommendation systems (e.g., Netflix), virtual assistants (e.g., Siri), autonomous vehicles, and medical diagnosis.']\n",
            "Generated Answer: Challenges of AI and ML include data privacy concerns, algorithmic bias, and the need for large datasets to train accurate models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3eH0x1y7SVWY"
      }
    }
  ]
}